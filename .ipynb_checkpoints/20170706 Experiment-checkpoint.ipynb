{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20170709 실험\n",
    "\n",
    "Scale normalization해서 학습한 것에 대해서 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "generator/g_h0_lin/Matrix:0 (float32_ref 100x8192) [819200, bytes: 3276800]\n",
      "generator/g_h0_lin/bias:0 (float32_ref 8192) [8192, bytes: 32768]\n",
      "generator/g_bn0/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "generator/g_bn0/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "generator/g_h1/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]\n",
      "generator/g_h1/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/g_bn1/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/g_bn1/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "generator/g_h2/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]\n",
      "generator/g_h2/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_bn2/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_bn2/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "generator/g_h3/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]\n",
      "generator/g_h3/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_bn3/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_bn3/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "generator/g_h4/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]\n",
      "generator/g_h4/biases:0 (float32_ref 3) [3, bytes: 12]\n",
      "discriminator/d_h0_conv/w:0 (float32_ref 5x5x3x64) [4800, bytes: 19200]\n",
      "discriminator/d_h0_conv/biases:0 (float32_ref 64) [64, bytes: 256]\n",
      "discriminator/d_h1_conv/w:0 (float32_ref 5x5x64x128) [204800, bytes: 819200]\n",
      "discriminator/d_h1_conv/biases:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_bn1/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_bn1/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "discriminator/d_h2_conv/w:0 (float32_ref 5x5x128x256) [819200, bytes: 3276800]\n",
      "discriminator/d_h2_conv/biases:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/d_bn2/beta:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/d_bn2/gamma:0 (float32_ref 256) [256, bytes: 1024]\n",
      "discriminator/d_h3_conv/w:0 (float32_ref 5x5x256x512) [3276800, bytes: 13107200]\n",
      "discriminator/d_h3_conv/biases:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/d_bn3/beta:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/d_bn3/gamma:0 (float32_ref 512) [512, bytes: 2048]\n",
      "discriminator/d_h4_lin/Matrix:0 (float32_ref 8192x1) [8192, bytes: 32768]\n",
      "discriminator/d_h4_lin/bias:0 (float32_ref 1) [1, bytes: 4]\n",
      "Total size of variables: 9451908\n",
      "Total bytes of variables: 37807632\n",
      " [*] Reading checkpoints...\n",
      "INFO:tensorflow:Restoring parameters from checkpoint/PatchofPlaces_128_64_64/DCGAN.model-58502\n",
      " [*] Success to read DCGAN.model-58502\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from model import DCGAN\n",
    "from utils import pp, visualize, to_json, show_all_variables\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import sys\n",
    "\n",
    "g_batch_size = 128\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_integer(\"epoch\", 25, \"Epoch to train [25]\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.0002, \"Learning rate of for adam [0.0002]\")\n",
    "flags.DEFINE_float(\"beta1\", 0.5, \"Momentum term of adam [0.5]\")\n",
    "flags.DEFINE_integer(\"train_size\", np.inf, \"The size of train images [np.inf]\")\n",
    "flags.DEFINE_integer(\"batch_size\", g_batch_size, \"The size of batch images [64]\")\n",
    "flags.DEFINE_integer(\"input_height\", 64, \"The size of image to use (will be center cropped). [108]\")\n",
    "flags.DEFINE_integer(\"input_width\", None,\n",
    "                     \"The size of image to use (will be center cropped). If None, same value as input_height [None]\")\n",
    "flags.DEFINE_integer(\"output_height\", 64, \"The size of the output images to produce [64]\")\n",
    "flags.DEFINE_integer(\"output_width\", None,\n",
    "                     \"The size of the output images to produce. If None, same value as output_height [None]\")\n",
    "flags.DEFINE_string(\"dataset\", \"PatchofPlaces\", \"The name of dataset [celebA, mnist, lsun]\")\n",
    "flags.DEFINE_string(\"input_fname_pattern\", \"*/*.jpg\", \"Glob pattern of filename of input images [*]\")\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoint\", \"Directory name to save the checkpoints [checkpoint]\")\n",
    "flags.DEFINE_string(\"sample_dir\", \"samples\", \"Directory name to save the image samples [samples]\")\n",
    "flags.DEFINE_boolean(\"train\", False, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"crop\", False, \"True for training, False for testing [False]\")\n",
    "flags.DEFINE_boolean(\"visualize\", False, \"True for visualizing, False for nothing [False]\")\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "\n",
    "pp.pprint(flags.FLAGS.__flags)\n",
    "\n",
    "if FLAGS.input_width is None:\n",
    "    FLAGS.input_width = FLAGS.input_height\n",
    "if FLAGS.output_width is None:\n",
    "    FLAGS.output_width = FLAGS.output_height\n",
    "\n",
    "if not os.path.exists(FLAGS.checkpoint_dir):\n",
    "    os.makedirs(FLAGS.checkpoint_dir)\n",
    "if not os.path.exists(FLAGS.sample_dir):\n",
    "    os.makedirs(FLAGS.sample_dir)\n",
    "\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "run_config = tf.ConfigProto()\n",
    "run_config.gpu_options.allow_growth = True\n",
    "\n",
    "\n",
    "sess = tf.Session(config=run_config)\n",
    "\n",
    "dcgan = DCGAN(\n",
    "    sess,\n",
    "    input_width=FLAGS.input_width,\n",
    "    input_height=FLAGS.input_height,\n",
    "    output_width=FLAGS.output_width,\n",
    "    output_height=FLAGS.output_height,\n",
    "    batch_size=FLAGS.batch_size,\n",
    "    sample_num=FLAGS.batch_size,\n",
    "    dataset_name=FLAGS.dataset,\n",
    "    input_fname_pattern=FLAGS.input_fname_pattern,\n",
    "    crop=FLAGS.crop,\n",
    "    checkpoint_dir=FLAGS.checkpoint_dir,\n",
    "    sample_dir=FLAGS.sample_dir)\n",
    "\n",
    "show_all_variables()\n",
    "\n",
    "if not dcgan.load(FLAGS.checkpoint_dir)[0]:\n",
    "    raise Exception(\"[!] Train a model first, then run test mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_extraction(dcgan, file_names):\n",
    "    return dcgan.get_feature(FLAGS, file_names)\n",
    "    \n",
    "def maxpooling(disc):\n",
    "#     pooling = [\n",
    "#         tf.nn.max_pool(disc[i],ksize=[1,2**(4-i),2**(4-i),1],\n",
    "#                        strides=[1,2**(4-i),2**(4-i),1],padding='SAME')\n",
    "#         for i in range(4)\n",
    "#     ]\n",
    "    pooling = tf.nn.max_pool(disc[3],ksize=[1,2**(4),2**(4),1],\n",
    "                        strides=[1,2**(4),2**(4),1],padding='SAME')\n",
    "\n",
    "    maxpool_result = sess.run(pooling)\n",
    "\n",
    "#     for idx in range(4):\n",
    "#         print(idx, maxpool_result[idx].shape)\n",
    "\n",
    "    return maxpool_result\n",
    "\n",
    "def flatten(disc):\n",
    "#     flatten = [\n",
    "#         tf.reshape(disc[i],[g_batch_size, -1])\n",
    "#         for i in range(4)\n",
    "#     ]\n",
    "    flatten = tf.reshape(disc, [g_batch_size, -1])\n",
    "    flatten_result = sess.run(flatten)\n",
    "    \n",
    "    return flatten_result\n",
    "\n",
    "def concat(disc):\n",
    "    concat = tf.concat(disc,1)\n",
    "\n",
    "    concat_result = sess.run(concat)\n",
    "    \n",
    "    return concat_result\n",
    "\n",
    "def feature_ext_GAN(file_names):\n",
    "    \n",
    "    ret = layer_extraction(dcgan, file_names)\n",
    "    ret = maxpooling(ret)\n",
    "    ret = flatten(ret)\n",
    "    ret = concat(ret)\n",
    "        \n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ~ 150\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d35d3b3785f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfile_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mfile_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# patch_path =\"/media/dongwonshin/Ubuntu Data/Datasets/Places365/Large_images/val_large/patches\"\n",
    "# data = glob(\"%s/Places365_val_%08d/*.jpg\" % (patch_path, idx))\n",
    "# output_filename = '/media/dongwonshin/Ubuntu Data/Datasets/Places365/Large_images/val_large/descs/20170702/' + (name.split('/')[-2])+'.desc'\n",
    "\n",
    "for term in [0,1]:\n",
    "    print('%d ~ %d' % (50*term,50*(term+1)))\n",
    "    \n",
    "    disc_list = []\n",
    "    batch_list = []\n",
    "    file_names = []\n",
    "    \n",
    "    patch_dirs_path = \"/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/City Centre ManualLC/patches\"\n",
    "#     patch_dirs_path = \"/home/dongwonshin/Desktop/DCGAN-tensorflow/data/PatchofPlaces\"\n",
    "    patch_dirs = sorted(glob(\"%s/*/\" % (patch_dirs_path)))\n",
    "    patch_dirs = patch_dirs[term*50:(term+1)*50]\n",
    "    \n",
    "    for patch_dir in patch_dirs:\n",
    "        data = sorted(glob(\"%s/*.jpg\" % (patch_dir)))\n",
    "        file_names.append(data)\n",
    "\n",
    "    file_names=np.concatenate(file_names)\n",
    "    print('total:',len(file_names))\n",
    "    print(file_names)\n",
    "\n",
    "    idx = 0\n",
    "    for idx in range(0, len(file_names)-g_batch_size,g_batch_size):\n",
    "        batch_files = file_names[idx: idx+g_batch_size]\n",
    "\n",
    "        disc = feature_ext_GAN(batch_files)\n",
    "        disc_list.append(disc)\n",
    "        batch_list.append(batch_files)\n",
    "        sys.stdout.write('.')\n",
    "\n",
    "    final_disc_list = np.concatenate(disc_list)\n",
    "    final_batch_list = np.concatenate(batch_list)\n",
    "    \n",
    "#     print(final_disc_list)\n",
    "#     print('\\nPerforming PCA..')\n",
    "#     X = np.array(final_disc_list)\n",
    "#     pca.fit(X)\n",
    "#     final_disc_list = pca.transform(X)\n",
    "    \n",
    "    for idx, name in enumerate(final_batch_list):\n",
    "#         output_filename = '/media/dongwonshin/Ubuntu Data/Datasets/Places365/Large_images/val_large/descs/20170709 Scale Normalization/' + (name.split('/')[-2])+'.desc'\n",
    "        output_filename = '/media/dongwonshin/Ubuntu Data/Datasets/FAB-MAP/Image Data/City Centre ManualLC/descs/20170709 Scale Normalization/' + (name.split('/')[-2])+'.desc'\n",
    "        with open(output_filename,'at') as fp:\n",
    "            for v in final_disc_list[idx]:\n",
    "                fp.write('%f ' % v)\n",
    "            fp.write('\\n')\n",
    "\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14976, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_disc_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## 2017년 07월 09일 14시 01분 47초에 추가:\n",
    "디버깅중..\n",
    "feature_ext_GAN함수안에서 \n",
    "각 단계별로 주석 처리해보고 돌려보고 안해보고 돌려봄..\n",
    "layer_extraction은 통과.\n",
    "맥스풀링 왜이렇게 오래 걸리지..\n",
    "\n",
    "내가 만든 부분 session.run 여러번 돌리면 죽는거 같은데..\n",
    "\n",
    "## 2017년 07월 09일 15시 01분 47초에 추가:\n",
    "피쳐만드는 부분을.. dcgan.get_feature함수 안에 넣어서.. dcgan 클래스의 session을 사용하도록 만들어서 돌려본다,..\n",
    "\n",
    "\n",
    "## 2017년 07월 09일 15시 10분 24초에 추가:\n",
    "그래도 안되는군..\n",
    "그래도 중간에 죽네..ㅅㅂ.. 아..\n",
    "그냥 일단 수작업으로... 수행..\n",
    "\n",
    "## 2017년 07월 09일 16시 17분 29초에 추가\n",
    "DCGAN 마지막 레이어의 값으로만 피쳐 만들기.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
